Model6 is the first working model, works with only one weight value and is not very stable

---Env Randomization---
Model7 converges to lesser loss values, but the agent doesnt learn anything
Model8 is same as model6, but trained more to see if it converges to a better solution (also
    bigger n_steps and batch_size) (its shit)
Model9 is trained for the same time, batch_size=64 and n_steps=1024 as Model6 but this time
    acceleration of body and EE stats are passed as observations
Model10 is the same as model9 but batch_size=256 and n_steps=8192 (also shit)
Model11, here we have increased the no of steps per episode from 1000 to 100_000.

---Env Randomization off---
Model12,13 are mistakes - env rewards were fucked up
Model14 is the same as model 11, but the training was interrupted on step 140k
Model15 - the negative reward for glitches has jacked up. Model is still shit.

---Gravity mod turned off---
Gravity was probably causing the markov assumption to not hold, so removed it.
Wrapped the env and normalized the observations
Started using tensorboard to monitor the progress of my RL algorithm
Model16 is the result of a few steps of training on this. Loss values were humorously low.
Model17 is another similar failure
Model18 and Model19 are also failures
Tuned some hyperparams for Model20 - here are the params:
    params = {
    'learning_rate': 3e-4,      # Learning rate for the optimizer
    'n_steps': 2048,            # Number of steps to run for each environment per update
    'batch_size': 64,           # Mini-batch size for each gradient update
    'n_epochs': 3,              # Number of optimization epochs to perform
    'gamma': 0.99,              # Discount factor for future rewards
    'gae_lambda': 0.95,         # GAE lambda parameter
    'ent_coef': 0.2,            # Coefficient for entropy bonus
    'vf_coef': 0.5,             # Coefficient for the value function loss
    'verbose': 1,               # Verbosity level for logging
    'tensorboard_log': 'ppo_logs/', # Directory to save logs
} - Failed
Decreased the episode size to 2048 - for Model21
After making the stupidest mistake in the history of RL, the reward function has finally
been fixed. Model22 is the first model to be trained on the fixed reward function.
Fixed anti-cheat and reward function again. Removed the contact constraint - Model23
---- Incremental control ----
Mode27 - First model to be trained with incremental control.
Model28 is very promising, with rewards of 4000+ - this however did not learn all the essentials
episodes were too long.
Model29 was trained for 5M steps, with shorter episodes
but the ent_coef was too high so the progress was unstable
Logs for model29 were in PPO15, Env randomization was on here.
Model30 fixed all above that was wrong in model29. Model30 was trained for 5M steps. Model30 
could have literaly byhearted the env but it failed to do even that.

Changed the reward function to just reward=1 if vel<10. Much simpler, and training it with no 
randomization in Model31. (direct control used)
model32 is same as above.

model33, model34 - simple 1 if vel<10 reward function. Incremental control with multiplier=30. Random=True
model35 - has epsize=2048, nsteps=256, batch=64, otherwise same as model34

--------------------
New idea - First we teach the model to not fucking break itself
Then we gradually teach it to reduce velocity.
model36 - rewarded for staying alive. bigger nn. control direct. I trained for 5M+ steps, absolutely no progress.

--------------------
Fixed the env glitches, random policy no longer breaks everything. At least robot stays in env.
However, now we can only use incremental control.
model37 worked. Quite well, might I add.
Turns out, the problem was that the required actions for stability ranged from 0.005 to -0.005, and sampling 
an action space from -1 to 1 led to updates that were too extreme. Random initialization took too long to
reach this range, and since the robot kept going out of bounds, it really learnt very slowly.
Renormalization fixed the issue.
the progress is logged in PPO-24
further training is done for 10M steps in PPO-26
I think further training broke model37. :(


model38 - reduction to the observation function. logs are in PPO_27
------------------------
The loss function has been bad. Like it gave a 50% reward to even random policy.
New loss function (function 1) and training with 9 observations in an extended NN.
PPO-28, model39. initially appears to converge in opp direction. (gets worse)
Eventually manages to get there. But reward is very low.

model40/ppo_29 will be trained on larger n_steps=8192 and batch size=256.

when randomization is on, env is not markov anymore.
Lets turn randomization of mass off and see.
model41/ppo_30 - Still didnt converge, at least not for 300k steps.

Lets try turning randomization off completely.
model42/ppo_31 - does not converge at all :(

random off, new reward function
model44/ppo_33 - Does not converge at all.

model48 converged, interesting strategy. It elongated to full length and chose to change 
stiffness to stabilize.

Awkwardly, after model reaches a peak, more training leads to it getting worse and staying worse.
Avoid training more than required.

Finally works - model50/ppo_39 
for robot6 - model52/ppo_41

FINAL - model54/ppo_43
--------------------------
